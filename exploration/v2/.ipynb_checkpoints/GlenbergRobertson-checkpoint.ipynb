{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"Tokenizer loaded\")\n",
    "\n",
    "model = transformers.BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'jimi', 'hendrix', '?', '[SEP]', 'he', 'was', 'a', 'great', '[MASK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jimi Hendrix? [SEP] He was a great guitarist [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 11\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "print(tokenized_text)\n",
    "# assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = tt.tensor([indexed_tokens])\n",
    "segments_tensors = tt.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1025,  999, 1012])\n",
      "[[';'], ['!'], ['.']]\n"
     ]
    }
   ],
   "source": [
    "# Predict all tokens\n",
    "with tt.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "predicted_index = tt.argsort(predictions[0, masked_index])[-3:]\n",
    "print(predicted_index)\n",
    "predicted_token = [tokenizer.convert_ids_to_tokens([ix]) for ix in predicted_index.numpy()]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.167552, 11.620032, 14.429442], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(predictions[0, masked_index])[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(setting, context):\n",
    "    \n",
    "    encoded_setting = [tokenizer.eos_token_id] + tokenizer.encode(setting)\n",
    "    encoded_critical = tokenizer.encode(critical)\n",
    "\n",
    "    input_ids = tt.tensor(encoded_setting + encoded_critical).unsqueeze(0)\n",
    "    \n",
    "    label_mask = tt.tensor(encoded_setting + encoded_critical)\n",
    "    label_mask[:len(encoded_setting)] = -1.0\n",
    "    \n",
    "    with tt.no_grad():\n",
    "        outputs = model(input_ids, labels=label_mask)\n",
    "        return outputs[:2] + (len(encoded_critical), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 13 17\n",
      "tensor(3.8504) tensor(65.4565) 17\n"
     ]
    }
   ],
   "source": [
    "setting = \"Marissa forgot to bring her pillow on her camping trip.\"\n",
    "critical = \"As a substitute for her pillow, she filled up an old sweater with leaves.\" # Afforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"As a substitute for her pillow, she filled up an old sweater with rocks.\" # Nonafforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"As a substitute for her pillow, she filled up an old sweater with some clothes.\" # Related\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2887) tensor(55.9079) 17\n",
      "tensor(3.3341) tensor(56.6801) 17\n",
      "tensor(3.1131) tensor(56.0363) 18\n"
     ]
    }
   ],
   "source": [
    "setting = \"Mike was freezing while walking up State Street into a brisk wind. He knew that he had to get his face covered pretty soon or he would get frostbite. Unfortunately, he didn't have enough money to buy a scarf.\"\n",
    "critical = \"Being clever, he walked into a store and bought a newspaper to cover his face.\" # Afforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"Being clever, he walked into a store and bought some candy to cover his face.\" # Nonafforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"Being clever, he walked into a store and bought a large hat to cover his face.\" # Related\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5960) tensor(50.3446) 14\n",
      "tensor(4.0550) tensor(52.7149) 13\n",
      "tensor(3.5957) tensor(50.3395) 14\n"
     ]
    }
   ],
   "source": [
    "setting = \"Jared went to the supermarket to get some groceries for dinner. Unfortunately, he left his grocery list at home.\"\n",
    "critical = \"Luckily, he found a paper bag to write a few items on.\" # Afforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"Luckily, he found some vegetables to write a few items on.\" # Nonafforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"Luckily, he found some scrap paper to write a few items on.\" # Related\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9109) tensor(46.9313) 12\n",
      "tensor(3.9812) tensor(47.7749) 12\n",
      "tensor(3.8577) tensor(46.2930) 12\n"
     ]
    }
   ],
   "source": [
    "setting = \"It was the second Tuesday, and Jackson had just sat down at the cafe waiting for Emilia. The barista disclaimed that they had just run out of mugs.\"\n",
    "critical = \"He offered to pour the latte in a bowl instead.\" # Afforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"He offered to pour the latte in a spoon instead.\" # Nonafforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"He offered to pour the latte in a glass instead.\" # Related\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9351) tensor(52.8318) 18\n",
      "tensor(2.9269) tensor(52.6840) 18\n",
      "tensor(2.8260) tensor(48.0421) 17\n"
     ]
    }
   ],
   "source": [
    "setting = \"Marianne welcomed Jacque in, and they warmly greeted each other. They walked into the kitchen, where Marianne made them coffee.\"\n",
    "critical = \"While she was doing that, Jacque took a seat on a nearby stepladder.\" # Afforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"While she was doing that, Jacque took a seat on a nearby glass of milk.\" # Nonafforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"While she was doing that, Jacque took a seat on a nearby armchair.\" # Related\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 19 29\n",
      "tensor(3.4817) tensor(100.9704) 29\n",
      "37 19 18\n",
      "tensor(3.2275) tensor(58.0946) 18\n",
      "37 19 18\n",
      "tensor(3.3239) tensor(59.8303) 18\n",
      "48 19 29\n",
      "tensor(3.4986) tensor(101.4608) 29\n"
     ]
    }
   ],
   "source": [
    "setting = \"Kate was cleaning her kitchen on Sunday morning after a big party she had the night before.\"\n",
    "critical = \"Since she couldn't reach the ceiling, she stuck her broom up in the air to try to get a piece of gum off her ceiling tile.\" # Afforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"She got down on her hands and knees to scrape the beer stains off the ceiling tile.\" # Nonafforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"She got down on her hands and knees to scrape the beer stains off the floor tile.\" # Afforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"Since she couldn't reach the ceiling, she stuck her broom up in the air to try to get a piece of gum off her floor tile.\" # Nonafforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 24 21\n",
      "tensor(3.6573) tensor(76.8030) 21\n",
      "43 24 19\n",
      "tensor(4.2905) tensor(81.5188) 19\n",
      "43 24 19\n",
      "tensor(4.0005) tensor(76.0103) 19\n",
      "45 24 21\n",
      "tensor(3.8633) tensor(81.1292) 21\n"
     ]
    }
   ],
   "source": [
    "setting = \"Marvin was doing some regular cleaning in his apartment, except the kitchen door kept closing inconveniently as he worked.\"\n",
    "\n",
    "# Obejct 1: piece of toast\n",
    "critical = \"His morning only got worse after he realized how hungry he was. He quickly ate a piece of toast.\" # Afforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"The looked around and saw a piece of toast, which he grabbed and jammed the door with.\" # Nonafforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "# Object 2: piece of plastic\n",
    "critical = \"The looked around and saw a piece of plastic, which he grabbed and jammed the door with.\" # Afforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)\n",
    "\n",
    "critical = \"His morning only got worse after he realized how hungry he was. He quickly ate a piece of plastic.\" # Nonafforded\n",
    "\n",
    "loss, logits, en = measure(setting, critical)\n",
    "print(loss, loss * en, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1217 14:58:54.362553 4451098048 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/alex/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1217 14:58:54.855477 4451098048 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/alex/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1217 14:58:54.864479 4451098048 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1217 14:58:55.248724 4451098048 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/alex/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PROCESSED = '../../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstancesData(Dataset):\n",
    "    \n",
    "    def encode(self, item):\n",
    "        padding = 64\n",
    "        \n",
    "        emb = model(tt.tensor([tokenizer.encode(item)]))[0][0, :padding, :]\n",
    "                \n",
    "        pad = tt.zeros((padding, 768))\n",
    "        pad[:emb.size()[0]] = emb\n",
    "        \n",
    "        return pad\n",
    "        \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.data = pd.read_csv(op.join(DATA_PROCESSED, 'labeled.csv'), nrows=50).dropna()\n",
    "        \n",
    "        self.data[\"embedding\"] = self.data[\"fragment\"].apply(self.encode)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        return {\n",
    "            \"embedding\": self.data[\"embedding\"].iloc[ix],\n",
    "            \"target\": tt.as_tensor(self.data[\"metaphorical\"].iloc[ix])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = InstancesData()\n",
    "\n",
    "loader = DataLoader(data, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(768, 16)\n",
    "        self.layer2 = nn.Linear(16, 1)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, batch):        \n",
    "        batch = self.layer1(batch)\n",
    "        batch = F.relu(batch)\n",
    "        batch = self.layer2(batch).flatten(start_dim=1)\n",
    "        batch = self.layer3(batch).flatten()\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = tt.optim.Adam(classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9a7f2378d2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    t_loss = 0.0\n",
    "\n",
    "    for i_batch, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = classifier(batch[\"embedding\"])\n",
    "\n",
    "        loss = tt.sum(tt.pow(tt.sigmoid(output) - batch[\"target\"].float(), 2))\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        t_loss += loss.item()\n",
    "\n",
    "    tl = t_loss / (i_batch + 1)\n",
    "    print(f\"[{epoch+1}] Loss: {tl:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyNLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
